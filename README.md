# Optimizing Lightweight Transformer Models: Training GPT-Tiny with Efficient Scheduling and Architecture Tweaks

• Trained a GPT-Tiny model incorporating weight tying and a cosine learning rate scheduler
to improve training efficiency and convergence.
• Conducted a comprehensive ablation study across 35 experimental runs, achieving a
perplexity below 50 and identifying key architectural and optimization parameters
contributing to performance gains.
