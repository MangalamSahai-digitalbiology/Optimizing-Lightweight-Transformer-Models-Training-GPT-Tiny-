output_dir: outputs/Q3.5_run2
tokenizer_encoding: gpt2
model_config:
  n_embd: 128
  n_head: 16
  n_positions: 64
  n_layer: 6
device: auto
batch_size: 128
seq_len: 64
num_warmup_steps: 10
num_training_steps: 4000
grad_accumulation_steps: 10
min_lr: 0.0001
max_lr: 0.0005
